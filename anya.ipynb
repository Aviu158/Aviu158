{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1szl8pVMjqX2myukp-PKKCjWhChbXvTSY",
      "authorship_tag": "ABX9TyN/UDlxHqzQIKXgeEhmN/LZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aviu158/Aviu158/blob/main/anya.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "AMNWmjiOlIgq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "225c4774-38e9-4c85-ecbc-86fdc8f91bd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.21.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from wandb) (25.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.14.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.7.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKdYfXRcn1Wj",
        "outputId": "18e4cb52-50b0-4d40-8c2b-1e715957b9a4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/dataset.zip -d /content/dataset/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yy6rCA12oBwz",
        "outputId": "bf1380da-bb04-402c-fa73-db8bd665573e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/dataset.zip\n",
            "replace /content/dataset/dataset/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cac028f"
      },
      "source": [
        "# Task\n",
        "Train a UNet model from scratch using PyTorch to generate colored polygons based on an input polygon image and a color name. The model should be trained on the dataset provided at \"dataset.zip\". Track experiments with wandb, demonstrate inference in a Jupyter Notebook, and provide a report detailing hyperparameters, architecture, and insights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27502855"
      },
      "source": [
        "## Load and preprocess data\n",
        "\n",
        "### Subtask:\n",
        "Load the training and validation data, including the input polygon images, output colored polygon images, and the data.json files. Preprocess the images and prepare the data for input to the UNet model, considering how to incorporate the color information.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83df9f38"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the base path, load the data.json files, and create lists of file paths and color information for training and validation datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e540432f"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because the path to the data.json file was incorrect. I need to fix the path to load the data correctly. The data is located in `/content/dataset/dataset/training/data.json` and `/content/dataset/dataset/validation/data.json`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRhG0MUlphCJ",
        "outputId": "ecbd75d6-5229-4f99-e542-f3ccd72985cb"
      },
      "source": [
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "\n",
        "# 1. Define the base path to the unzipped dataset\n",
        "base_path = '/content/dataset/dataset'\n",
        "train_path = os.path.join(base_path, 'training')\n",
        "val_path = os.path.join(base_path, 'validation')\n",
        "\n",
        "# 2. Load the data.json files\n",
        "with open(os.path.join(train_path, 'data.json'), 'r') as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "with open(os.path.join(val_path, 'data.json'), 'r') as f:\n",
        "    val_data = json.load(f)\n",
        "\n",
        "# 3. Create lists of file paths and color information using the correct keys\n",
        "train_input_paths = [os.path.join(train_path, 'inputs', item['input_polygon']) for item in train_data]\n",
        "train_output_paths = [os.path.join(train_path, 'outputs', item['output_image']) for item in train_data]\n",
        "train_colors = [item['colour'] for item in train_data]\n",
        "\n",
        "val_input_paths = [os.path.join(val_path, 'inputs', item['input_polygon']) for item in val_data]\n",
        "val_output_paths = [os.path.join(val_path, 'outputs', item['output_image']) for item in val_data]\n",
        "val_colors = [item['colour'] for item in val_data]\n",
        "\n",
        "# Get all unique colors to create a mapping for encoding\n",
        "all_colors = sorted(list(set(train_colors + val_colors)))\n",
        "color_to_int = {color: i for i, color in enumerate(all_colors)}\n",
        "num_colors = len(all_colors)\n",
        "\n",
        "# 4. Implement a custom PyTorch Dataset class\n",
        "class PolygonDataset(Dataset):\n",
        "    def __init__(self, input_paths, output_paths, colors, color_to_int, image_size=(128, 128)):\n",
        "        self.input_paths = input_paths\n",
        "        self.output_paths = output_paths\n",
        "        self.colors = colors\n",
        "        self.color_to_int = color_to_int\n",
        "        self.image_size = image_size\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_img = Image.open(self.input_paths[idx]).convert('RGB')\n",
        "        output_img = Image.open(self.output_paths[idx]).convert('RGB')\n",
        "        color = self.colors[idx]\n",
        "\n",
        "        input_tensor = self.transform(input_img)\n",
        "        output_tensor = self.transform(output_img)\n",
        "\n",
        "        # Encode color using one-hot encoding\n",
        "        color_tensor = torch.zeros(len(self.color_to_int))\n",
        "        color_tensor[self.color_to_int[color]] = 1\n",
        "\n",
        "        return input_tensor, output_tensor, color_tensor\n",
        "\n",
        "# 5. Create instances of your custom Dataset\n",
        "train_dataset = PolygonDataset(train_input_paths, train_output_paths, train_colors, color_to_int)\n",
        "val_dataset = PolygonDataset(val_input_paths, val_output_paths, val_colors, color_to_int)\n",
        "\n",
        "# 6. Create PyTorch DataLoader instances\n",
        "batch_size = 16 # Define an appropriate batch size\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
        "print(f\"Number of unique colors: {num_colors}\")\n",
        "\n",
        "# Example of accessing a batch\n",
        "# input_batch, output_batch, color_batch = next(iter(train_dataloader))\n",
        "# print(f\"Input batch shape: {input_batch.shape}\")\n",
        "# print(f\"Output batch shape: {output_batch.shape}\")\n",
        "# print(f\"Color batch shape: {color_batch.shape}\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 56\n",
            "Number of validation samples: 5\n",
            "Number of unique colors: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d32e207c"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because the keys in the `data.json` file were not 'input' and 'output'. I need to inspect the structure of the JSON data to identify the correct keys for the input and output image filenames. Then, I will correct the code to use the correct keys and continue with creating the lists of file paths and color information.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36d5e8ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9c6a6b8-af91-4193-c286-85728a6c5bfb"
      },
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "base_path = '/content/dataset/dataset/'\n",
        "train_path = os.path.join(base_path, 'training')\n",
        "\n",
        "with open(os.path.join(train_path, 'data.json'), 'r') as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "# Display the first 5 items to understand the structure\n",
        "print(json.dumps(train_data[:5], indent=2))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"input_polygon\": \"octagon.png\",\n",
            "    \"colour\": \"cyan\",\n",
            "    \"output_image\": \"cyan_octagon.png\"\n",
            "  },\n",
            "  {\n",
            "    \"input_polygon\": \"square.png\",\n",
            "    \"colour\": \"purple\",\n",
            "    \"output_image\": \"purple_square.png\"\n",
            "  },\n",
            "  {\n",
            "    \"input_polygon\": \"hexagon.png\",\n",
            "    \"colour\": \"magenta\",\n",
            "    \"output_image\": \"magenta_hexagon.png\"\n",
            "  },\n",
            "  {\n",
            "    \"input_polygon\": \"square.png\",\n",
            "    \"colour\": \"magenta\",\n",
            "    \"output_image\": \"magenta_square.png\"\n",
            "  },\n",
            "  {\n",
            "    \"input_polygon\": \"hexagon.png\",\n",
            "    \"colour\": \"purple\",\n",
            "    \"output_image\": \"purple_hexagon.png\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7529719d"
      },
      "source": [
        "## Implement the UNet model\n",
        "\n",
        "### Subtask:\n",
        "Define the architecture of the UNet model from scratch using PyTorch. The model should be able to accept both the image input and the color information.\n",
        "\n",
        "**Reasoning**:\n",
        "Implement the UNet architecture with an encoder-decoder structure and skip connections. The color information needs to be incorporated into the model, possibly by concatenating the one-hot encoded color vector to the feature maps at some point in the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd8546b2"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_colors):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.enc_conv1 = self.double_conv(in_channels + num_colors, 64)\n",
        "        self.enc_conv2 = self.double_conv(64, 128)\n",
        "        self.enc_conv3 = self.double_conv(128, 256)\n",
        "        self.enc_conv4 = self.double_conv(256, 512)\n",
        "\n",
        "        # Decoder\n",
        "        self.dec_upconv1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.dec_conv1 = self.double_conv(512, 256)\n",
        "        self.dec_upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.dec_conv2 = self.double_conv(256, 128)\n",
        "        self.dec_upconv3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.dec_conv3 = self.double_conv(128, 64)\n",
        "\n",
        "        # Output layer\n",
        "        self.out_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def double_conv(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, color):\n",
        "        # Expand color tensor to match spatial dimensions of image\n",
        "        color_expanded = color.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, x.size(2), x.size(3))\n",
        "        x = torch.cat([x, color_expanded], dim=1)\n",
        "\n",
        "        # Encoder\n",
        "        enc1 = self.enc_conv1(x)\n",
        "        enc2 = self.maxpool(enc1)\n",
        "        enc2 = self.enc_conv2(enc2)\n",
        "        enc3 = self.maxpool(enc2)\n",
        "        enc3 = self.enc_conv3(enc3)\n",
        "        enc4 = self.maxpool(enc3)\n",
        "        enc4 = self.enc_conv4(enc4)\n",
        "\n",
        "        # Decoder\n",
        "        dec1 = self.dec_upconv1(enc4)\n",
        "        dec1 = torch.cat([dec1, enc3], dim=1)\n",
        "        dec1 = self.dec_conv1(dec1)\n",
        "        dec2 = self.dec_upconv2(dec1)\n",
        "        dec2 = torch.cat([dec2, enc2], dim=1)\n",
        "        dec2 = self.dec_conv2(dec2)\n",
        "        dec3 = self.dec_upconv3(dec2)\n",
        "        dec3 = torch.cat([dec3, enc1], dim=1)\n",
        "        dec3 = self.dec_conv3(dec3)\n",
        "\n",
        "        out = self.out_conv(dec3)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Example usage:\n",
        "# Assuming input image is 3 channels (RGB) and output is 3 channels (RGB)\n",
        "# num_colors is the number of unique colors\n",
        "# model = UNet(in_channels=3, out_channels=3, num_colors=num_colors)\n",
        "# print(model)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffdef168"
      },
      "source": [
        "## Set up training\n",
        "\n",
        "### Subtask:\n",
        "Define the loss function, optimizer, and training loop. Integrate wandb for experiment tracking.\n",
        "\n",
        "**Reasoning**:\n",
        "Initialize wandb to track the training progress. Define an appropriate loss function for image generation tasks (e.g., MSE or L1 loss) and an optimizer (e.g., Adam). Create a training loop that iterates through the data, performs forward and backward passes, and logs metrics to wandb."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbd16b71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "aab13e91-45b5-43d7-fe5e-f641f373ba70"
      },
      "source": [
        "import wandb\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "# 1. Initialize wandb\n",
        "# Replace 'your_project_name' with the name you want for your wandb project\n",
        "# You might need to log in to wandb in the notebook if you haven't already\n",
        "wandb.login()\n",
        "wandb.init(project='colored-polygon-unet')\n",
        "\n",
        "# 2. Define the model, loss function, and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = UNet(in_channels=3, out_channels=3, num_colors=num_colors).to(device)\n",
        "criterion = nn.MSELoss() # Using Mean Squared Error as the loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001) # Using Adam optimizer\n",
        "\n",
        "# 3. Define the training loop\n",
        "def train_model(model, dataloader, criterion, optimizer, num_epochs=5):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        for inputs, outputs, colors in dataloader:\n",
        "            inputs, outputs, colors = inputs.to(device), outputs.to(device), colors.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            predicted_outputs = model(inputs, colors)\n",
        "            loss = criterion(predicted_outputs, outputs)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloader.dataset)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "        # Log the training loss to wandb\n",
        "        wandb.log({\"train_loss\": epoch_loss})\n",
        "\n",
        "        running_loss = 0.0 # Reset running loss for the next epoch\n",
        "\n",
        "# 4. Define evaluation function (optional but recommended)\n",
        "def evaluate_model(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, outputs, colors in dataloader:\n",
        "            inputs, outputs, colors = inputs.to(device), outputs.to(device), colors.to(device)\n",
        "\n",
        "            predicted_outputs = model(inputs, colors)\n",
        "            loss = criterion(predicted_outputs, outputs)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    print(f\"Validation Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    # Log the validation loss to wandb\n",
        "    wandb.log({\"val_loss\": epoch_loss})\n",
        "\n",
        "# Example of how to call the training function:\n",
        "# train_model(model, train_dataloader, criterion, optimizer, num_epochs=10)\n",
        "\n",
        "# After training, you can save the model\n",
        "# torch.save(model.state_dict(), 'unet_colored_polygon.pth')\n",
        "\n",
        "# To finish the wandb run (usually after training)\n",
        "# wandb.finish()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250803_173905-uginijbw</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/adityayadav2788-central-university-of-jammu/colored-polygon-unet/runs/uginijbw' target=\"_blank\">divine-dust-5</a></strong> to <a href='https://wandb.ai/adityayadav2788-central-university-of-jammu/colored-polygon-unet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/adityayadav2788-central-university-of-jammu/colored-polygon-unet' target=\"_blank\">https://wandb.ai/adityayadav2788-central-university-of-jammu/colored-polygon-unet</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/adityayadav2788-central-university-of-jammu/colored-polygon-unet/runs/uginijbw' target=\"_blank\">https://wandb.ai/adityayadav2788-central-university-of-jammu/colored-polygon-unet/runs/uginijbw</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49b3e775",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "outputId": "a9f7a3b5-2ca4-46a1-d7cf-e478c004967f"
      },
      "source": [
        "# Start the training\n",
        "train_model(model, train_dataloader, criterion, optimizer, num_epochs=10)\n",
        "\n",
        "# Evaluate the model after training (optional)\n",
        "evaluate_model(model, val_dataloader, criterion)\n",
        "\n",
        "# After training, you can save the model\n",
        "torch.save(model.state_dict(), 'unet_colored_polygon.pth')\n",
        "\n",
        "# Finish the wandb run\n",
        "wandb.finish()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.8008\n",
            "Epoch 2/10, Loss: 0.4724\n",
            "Epoch 3/10, Loss: 0.1928\n",
            "Epoch 4/10, Loss: 0.2188\n",
            "Epoch 5/10, Loss: 0.1781\n",
            "Epoch 6/10, Loss: 0.1206\n",
            "Epoch 7/10, Loss: 0.1325\n",
            "Epoch 8/10, Loss: 0.1059\n",
            "Epoch 9/10, Loss: 0.1067\n",
            "Epoch 10/10, Loss: 0.1028\n",
            "Validation Loss: 0.0718\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>█▅▂▂▂▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>0.10283</td></tr><tr><td>val_loss</td><td>0.07177</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">divine-dust-5</strong> at: <a href='https://wandb.ai/adityayadav2788-central-university-of-jammu/colored-polygon-unet/runs/uginijbw' target=\"_blank\">https://wandb.ai/adityayadav2788-central-university-of-jammu/colored-polygon-unet/runs/uginijbw</a><br> View project at: <a href='https://wandb.ai/adityayadav2788-central-university-of-jammu/colored-polygon-unet' target=\"_blank\">https://wandb.ai/adityayadav2788-central-university-of-jammu/colored-polygon-unet</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250803_173905-uginijbw/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "821becd0"
      },
      "source": [
        "## Implement Inference and Testing\n",
        "\n",
        "### Subtask:\n",
        "Create a separate section or notebook to demonstrate how to load the trained model, provide input polygon images and color names, and generate and visualize the output colored polygon images.\n",
        "\n",
        "**Reasoning**:\n",
        "Load the trained model state dictionary. Prepare a function to take an input image path and a color name, preprocess them, run inference through the model, and postprocess the output to visualize the generated colored polygon image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b2664da"
      },
      "source": [
        "# Assuming you saved the model weights as 'unet_colored_polygon.pth' after training\n",
        "# If not, you can save the model state_dict after the training cell runs successfully\n",
        "# torch.save(model.state_dict(), 'unet_colored_polygon.pth')\n",
        "\n",
        "# Load the saved model state dictionary\n",
        "model = UNet(in_channels=3, out_channels=3, num_colors=num_colors).to(device)\n",
        "model.load_state_dict(torch.load('unet_colored_polygon.pth'))\n",
        "model.eval() # Set the model to evaluation mode\n",
        "\n",
        "def predict_colored_polygon(model, input_image_path, color_name, color_to_int, image_size=(128, 128), background_color=(255, 255, 255)):\n",
        "    # Load and preprocess the input image\n",
        "    input_img = Image.open(input_image_path).convert('RGB')\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(image_size),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    input_tensor = transform(input_img).unsqueeze(0).to(device) # Add batch dimension and move to device\n",
        "\n",
        "    # Encode the color\n",
        "    color_tensor = torch.zeros(len(color_to_int))\n",
        "    color_tensor[color_to_int[color_name]] = 1\n",
        "    color_tensor = color_tensor.unsqueeze(0).to(device) # Add batch dimension and move to device\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        predicted_output = model(input_tensor, color_tensor)\n",
        "\n",
        "    # Postprocess the output\n",
        "    predicted_output = predicted_output.squeeze(0).cpu() # Remove batch dimension and move to CPU\n",
        "    predicted_output_img = transforms.ToPILImage()(predicted_output)\n",
        "\n",
        "    # Change background color using NumPy for mask creation\n",
        "    generated_with_background = Image.new(\"RGB\", predicted_output_img.size, background_color)\n",
        "\n",
        "    # Convert PIL image to NumPy array\n",
        "    predicted_output_np = np.array(predicted_output_img)\n",
        "\n",
        "    # Create a mask: True where the pixel is NOT black (assuming black is the background)\n",
        "    # A pixel is considered \"not black\" if the sum of its RGB values is greater than a small threshold\n",
        "    mask_np = np.sum(predicted_output_np, axis=2) > 10 # Use a small threshold to account for potential minor variations\n",
        "\n",
        "    # Convert the boolean NumPy mask to a PIL '1' mode image mask\n",
        "    mask_pil = Image.fromarray(mask_np.astype(np.uint8) * 255, mode='L')\n",
        "\n",
        "    # Paste the generated image onto the background using the created mask\n",
        "    generated_with_background.paste(predicted_output_img, (0, 0), mask_pil)\n",
        "\n",
        "    return generated_with_background # Return the image with the new background\n",
        "\n",
        "# Example usage:\n",
        "# Replace 'path/to/your/input_polygon.png' with the actual path to an input image\n",
        "# Replace 'desired_color' with a color name from your dataset (e.g., 'red')\n",
        "# input_image_example_path = '/content/dataset/dataset/validation/inputs/square.png'\n",
        "# desired_color_example = 'red'\n",
        "\n",
        "# generated_image = predict_colored_polygon(model, input_image_example_path, desired_color_example, color_to_int)\n",
        "\n",
        "# Display the generated image (optional)\n",
        "# generated_image.show() # In a script\n",
        "# display(generated_image) # In a Jupyter Notebook or Colab"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "1529e195",
        "outputId": "f2b9932c-8090-4c48-bb89-699d940c0a79"
      },
      "source": [
        "from IPython.display import display\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Define the UNet class (copied from cell fd8546b2)\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_colors):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.enc_conv1 = self.double_conv(in_channels + num_colors, 64)\n",
        "        self.enc_conv2 = self.double_conv(64, 128)\n",
        "        self.enc_conv3 = self.double_conv(128, 256)\n",
        "        self.enc_conv4 = self.double_conv(256, 512)\n",
        "\n",
        "        # Decoder\n",
        "        self.dec_upconv1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.dec_conv1 = self.double_conv(512, 256)\n",
        "        self.dec_upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.dec_conv2 = self.double_conv(256, 128)\n",
        "        self.dec_upconv3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.dec_conv3 = self.double_conv(128, 64)\n",
        "\n",
        "        # Output layer\n",
        "        self.out_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def double_conv(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, color):\n",
        "        # Expand color tensor to match spatial dimensions of image\n",
        "        color_expanded = color.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, x.size(2), x.size(3))\n",
        "        x = torch.cat([x, color_expanded], dim=1)\n",
        "\n",
        "        # Encoder\n",
        "        enc1 = self.enc_conv1(x)\n",
        "        enc2 = self.maxpool(enc1)\n",
        "        enc2 = self.enc_conv2(enc2)\n",
        "        enc3 = self.maxpool(enc2)\n",
        "        enc3 = self.enc_conv3(enc3)\n",
        "        enc4 = self.maxpool(enc3)\n",
        "        enc4 = self.enc_conv4(enc4)\n",
        "\n",
        "        # Decoder\n",
        "        dec1 = self.dec_upconv1(enc4)\n",
        "        dec1 = torch.cat([dec1, enc3], dim=1)\n",
        "        dec1 = self.dec_conv1(dec1)\n",
        "        dec2 = self.dec_upconv2(dec1)\n",
        "        dec2 = torch.cat([dec2, enc2], dim=1)\n",
        "        dec2 = self.dec_conv2(dec2)\n",
        "        dec3 = self.dec_upconv3(dec2)\n",
        "        dec3 = torch.cat([dec3, enc1], dim=1)\n",
        "        dec3 = self.dec_conv3(dec3)\n",
        "\n",
        "        out = self.out_conv(dec3)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "# Load data to get num_colors and color_to_int\n",
        "base_path = '/content/dataset/dataset'\n",
        "train_path = os.path.join(base_path, 'training')\n",
        "val_path = os.path.join(base_path, 'validation')\n",
        "\n",
        "with open(os.path.join(train_path, 'data.json'), 'r') as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "with open(os.path.join(val_path, 'data.json'), 'r') as f:\n",
        "    val_data = json.load(f)\n",
        "\n",
        "train_colors = [item['colour'] for item in train_data]\n",
        "val_colors = [item['colour'] for item in val_data]\n",
        "\n",
        "all_colors = sorted(list(set(train_colors + val_colors)))\n",
        "color_to_int = {color: i for i, color in enumerate(all_colors)}\n",
        "num_colors = len(all_colors)\n",
        "\n",
        "# Print statements to check if num_colors and color_to_int are defined\n",
        "print(f\"num_colors: {num_colors}\")\n",
        "print(f\"color_to_int: {color_to_int}\")\n",
        "\n",
        "# Assuming you saved the model weights as 'unet_colored_polygon.pth' after training\n",
        "# If not, you can save the model state_dict after the training cell runs successfully\n",
        "# torch.save(model.state_dict(), 'unet_colored_polygon.pth')\n",
        "\n",
        "# Load the saved model state dictionary\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = UNet(in_channels=3, out_channels=3, num_colors=num_colors).to(device)\n",
        "model.load_state_dict(torch.load('unet_colored_polygon.pth'))\n",
        "model.eval() # Set the model to evaluation mode\n",
        "\n",
        "def predict_colored_polygon(model, input_image_path, color_name, color_to_int, image_size=(128, 128), background_color=(255, 255, 255)):\n",
        "    # Load and preprocess the input image\n",
        "    input_img = Image.open(input_image_path).convert('RGB')\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(image_size),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    input_tensor = transform(input_img).unsqueeze(0).to(device) # Add batch dimension and move to device\n",
        "\n",
        "    # Encode the color\n",
        "    color_tensor = torch.zeros(len(color_to_int))\n",
        "    color_tensor[color_to_int[color_name]] = 1\n",
        "    color_tensor = color_tensor.unsqueeze(0).to(device) # Add batch dimension and move to device\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        predicted_output = model(input_tensor, color_tensor)\n",
        "\n",
        "    # Postprocess the output\n",
        "    predicted_output = predicted_output.squeeze(0).cpu() # Remove batch dimension and move to CPU\n",
        "    predicted_output_img = transforms.ToPILImage()(predicted_output)\n",
        "\n",
        "    # Change background color using NumPy for mask creation\n",
        "    generated_with_background = Image.new(\"RGB\", predicted_output_img.size, background_color)\n",
        "\n",
        "    # Convert PIL image to NumPy array\n",
        "    predicted_output_np = np.array(predicted_output_img)\n",
        "\n",
        "    # Create a mask: True where the pixel is NOT black (assuming black is the background)\n",
        "    # A pixel is considered \"not black\" if the sum of its RGB values is greater than a small threshold\n",
        "    mask_np = np.sum(predicted_output_np, axis=2) > 10 # Use a small threshold to account for potential minor variations\n",
        "\n",
        "    # Convert the boolean NumPy mask to a PIL '1' mode image mask\n",
        "    mask_pil = Image.fromarray(mask_np.astype(np.uint8) * 255, mode='L')\n",
        "\n",
        "    # Paste the generated image onto the background using the created mask\n",
        "    generated_with_background.paste(predicted_output_img, (0, 0), mask_pil)\n",
        "\n",
        "\n",
        "    return generated_with_background # Return the image with the new background\n",
        "\n",
        "# Example usage:\n",
        "# Replace 'path/to/your/input_polygon.png' with the actual path to an input image\n",
        "# Replace 'desired_color' with a color name from your dataset (e.g., 'red')\n",
        "input_image_example_path = '/content/dataset/dataset/training/inputs/octagon.png'\n",
        "desired_color_example = 'yellow'\n",
        "desired_background_color = (255, 255, 255) # White background\n",
        "\n",
        "generated_image = predict_colored_polygon(model, input_image_example_path, desired_color_example, color_to_int)\n",
        "\n",
        "# Display the generated image\n",
        "print(f\"Generated image for input '{input_image_example_path}' with color '{desired_color_example}':\")\n",
        "display(generated_image)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_colors: 8\n",
            "color_to_int: {'blue': 0, 'cyan': 1, 'green': 2, 'magenta': 3, 'orange': 4, 'purple': 5, 'red': 6, 'yellow': 7}\n",
            "Generated image for input '/content/dataset/dataset/training/inputs/octagon.png' with color 'yellow':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1312539152.py:138: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  mask_pil = Image.fromarray(mask_np.astype(np.uint8) * 255, mode='L')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=128x128>"
            ],
            "image/png": "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\n",
            "image/jpeg": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}